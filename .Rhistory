N_step <- length(Lambda)
step <- 1;
Alpha <- seq(from = 1, to = 0.05, by = -0.05)
#alpha <- 0.5 #Initialize alpha
nAlpha <- length(Alpha);
MSEcv <- mat.or.vec((N_step*nAlpha),4)
MSEeachAlpha <- mat.or.vec(nAlpha,4) # minimum MSE for each alpha
MeanSqErr <- mat.or.vec(nFolds,1)
SSE1Alpha	<- matrix(1e10,N_step,2)# temp matrix to keep MSE + std in each step
cvoutput <- foreach (i_alpha = 1:nAlpha, .combine = rbind, .packages = c("foreach")) %dopar% {
alpha <- Alpha[i_alpha]
SSE1Alpha <- matrix(1e10,N_step,2)# temp matrix to keep MSE + std in each step
cat("Testing alpha", i_alpha, "/",nAlpha,":\t\talpha: ",alpha,"\n")
foreach (i_s = 1:N_step, .combine = rbind) %dopar% {
lambda <- Lambda[i_s];
min_index <- which.min(SSE1Alpha[1:(i_s -1),1]);
previousL <- SSE1Alpha[min_index,1] + SSE1Alpha[min_index,2];
cat("\tTesting step", step, "\t\tlambda: ",lambda,"\t")
foreach (i = 1:nFolds, .combine = rbind, .packages = c("EBEN")) %dopar% {
#cat("Testing fold", j, "\n")
index <- which(foldId!=i)
Basis.Train <- BASIS[index,]
Target.Train <- Target[index]
index <- which(foldId == i)
Basis.Test <- BASIS[index,]
Target.Test <- Target[index]
SimF2fEB <- EBelasticNet.Gaussian(Basis.Train,Target.Train,lambda,alpha,Epis)
M <- length(SimF2fEB$weight)/6
Betas <- matrix(SimF2fEB$weight,nrow= M,ncol =6, byrow= FALSE)
Mu <- Betas[,3]
Mu0 <- SimF2fEB$Intercept[1]
#if(is.na(Mu0)){break}
ntest <- nrow(Basis.Test)
basisTest <- matrix(rep(0,ntest*M),ntest,M)
for(i_basis in 1:M){
loc1 <- Betas[i_basis,1]
loc2 <- Betas[i_basis,2]
if(loc1 !=0)
{
if(loc1==loc2){
basisTest[,i_basis] <- Basis.Test[,loc1]
}
else{
basisTest[,i_basis] <- Basis.Test[,loc1] * Basis.Test[,loc2]
}
}else{
basisTest <- rep(0,length(Target.Test))
}
}
#compute mean square error:
temp <- Target.Test - (Mu0 + basisTest%*%Mu)
MeanSqErr[i] <- t(temp)%*%temp
return(MeanSqErr[i])
}
SSE1Alpha[i_s,] <- c(mean(MeanSqErr),sd(MeanSqErr)/sqrt(nFolds))
cat("SSE",mean(MeanSqErr),"\n")
MSEcv[step,] <- c(alpha,lambda,mean(MeanSqErr),sd(MeanSqErr)/sqrt(nFolds))
out <- c(alpha,lambda,mean(MeanSqErr),sd(MeanSqErr)/sqrt(nFolds))
currentL <- MSEcv[step,3]
step <- step + 1;
# break out of 2nd for loop
#if((currentL - previousL)>0){break}
return(out)
}
#index <- which.min(SSE1Alpha[,1]);
#lambda <- Lambda[index];
#MSEeachAlpha[i_alpha,] <- c(alpha,lambda, SSE1Alpha[index,]);
}
cvdf <- as.data.frame(cvoutput)
colnames(cvdf) <- c("alpha", "lambda", "MSE", "StdErr")
rownames(cvdf) <- NULL
optimalindex <- which.min(cvdf$MSE)
optimalstats <- cvdf[optimalindex,]
result <- list(cvdf, optimalstats, optimalstats$alpha, optimalstats$lambda, optimalstats$MSE)
names(result) <- c("CrossValidation", "Optimal_Stats", "Alpha_Optimal", "Lambda_Optimal", "Minimum_MSE")
return(result)
}
else{
cat("Currently, only the binomial and gaussian prior distributions are allowed.")
}
}
sessionInfo()
#library(doParallel)
#library(foreach)
require(EBEN)
parEBEN.cv.doParallel <- function (BASIS, Target, nFolds, Epis = "no", foldId = 0, prior = "binomial"){
if(prior == "binomial"){
cat("Parallel Empirical Bayesian Elastic Net Linear Model (w/ Binomial Prior), Epis: ", Epis, ", ", nFolds,"-fold cross-validation\n",sep="")
N <- nrow(BASIS)
K <- ncol(BASIS)
set.seed(1)
if (length(foldId) != N) {
if (N%%nFolds != 0) {
foldId <- sample(c(rep(1:nFolds, floor(N/nFolds)), 1:(N%%nFolds)), N)
}
else {
foldId <- sample(rep(1:nFolds, floor(N/nFolds)), N)
}
}
lambda_Max <- log(1.1)
response <- Target - mean(Target)
response <- response/sqrt(sum(response * response))
for(i_b in 1:K){
basis 			= BASIS[,i_b];
basis 			= basis/sum(basis*basis);
corBy 			= basis%*%response;
if(corBy>lambda_Max) lambda_Max = corBy;
}
if(Epis == "yes"){
Alpha 				= seq(from = 1, to = 0.05, by = -0.1)
for(i_b in 1:(K-1)){
for(i_bj in (i_b + 1):K){
basis 	= BASIS[,i_b]*BASIS[,i_bj];
basis 	= basis/sum(basis*basis);
corBy 	= basis%*%(Target-mean(Target));
if(corBy>lambda_Max) lambda_Max = corBy;
}
}
}
#corBy <- foreach(i_b = 1:K, .packages = c("foreach"), .combine = "max") %dopar% {
#  basis <- BASIS[, i_b]
#  basis <- basis/sum(basis * basis)
#  corBy <- basis %*% response
#  }
#if (corBy > lambda_Max){
#  lambda_Max <- corBy
#}
#if (Epis == "yes") {
#  Alpha <- seq(from = 1, to = 0.05, by = -0.1)
#  corBy <- foreach(i_b = 1:(K - 1), .combine = "max") %dopar% {
#    foreach (i_bj = (i_b + 1):K, .combine = "max") %dopar% {
#      basis <- BASIS[, i_b] * BASIS[, i_bj]
#      basis <- basis/sum(basis * basis)
#      corBy <- basis %*% (Target - mean(Target))
#    }
#  }
#  if(corBy > lambda_Max) {
#    lambda_Max <- corBy
#  }
#}
lambda_Max <- lambda_Max * 10
lambda_Min <- log(0.001 * lambda_Max)
step <- (log(lambda_Max) - lambda_Min)/19
Lambda <- exp(seq(from = log(lambda_Max), to = lambda_Min, by = -step))
N_step <- length(Lambda)
step <- 1
if (Epis == "no") {
Alpha <- seq(from = 1, to = 0.05, by = -0.05)
}
nAlpha <- length(Alpha)
Likelihood <- mat.or.vec((N_step * nAlpha), 4)
MSEeachAlpha <- mat.or.vec(nAlpha, 4)
logL <- mat.or.vec(nFolds, 1)
SSE1Alpha <- matrix(1e+10, N_step, 2)
cvoutput <- foreach(i_alpha = 1:nAlpha, .combine= "rbind", .packages = c("foreach")) %dopar% {
alpha <- Alpha[i_alpha]
SSE1Alpha <- matrix(1e+10, N_step, 2)
cat("Testing alpha", i_alpha, "/", nAlpha, ":\t\talpha: ", alpha, "\n")
foreach(i_s = 1:N_step, .combine = "rbind") %dopar% {
lambda <- Lambda[i_s]
min_index <- which.min(SSE1Alpha[1:(i_s - 1), 1])
previousL <- SSE1Alpha[min_index, 1] + SSE1Alpha[min_index, 2]
cat("\tTesting step", step, "\t\tlambda: ", lambda, "\n")
foreach(i = 1:nFolds, .combine = rbind, .packages = c("EBEN")) %dopar% {
index <- which(foldId != i)
Basis.Train <- BASIS[index, ]
Target.Train <- Target[index]
index <- which(foldId == i)
Basis.Test <- BASIS[index, ]
Target.Test <- Target[index]
SimF2fEB <- EBEN::EBelasticNet.Binomial(Basis.Train, Target.Train, lambda, alpha, Epis, verbose = 0)
M <- length(SimF2fEB$weight)/6
Betas <- matrix(SimF2fEB$weight, nrow = M, ncol = 6, byrow = FALSE)
Mu <- Betas[, 3]
Mu0 <- SimF2fEB$Intercept[1]
rm(list = "SimF2fEB")
ntest <- nrow(Basis.Test)
if (M == 1 && Betas[1, 1] == 0) {
logL[i] = 0
}
else {
basisTest <- matrix(rep(0, ntest * M), ntest, M)
for(i_basis in 1:M) {
loc1 <- Betas[i_basis, 1]
loc2 <- Betas[i_basis, 2]
if (loc1 == loc2) {
basisTest[, i_basis] <- Basis.Test[, loc1]
}
else {
basisTest[, i_basis] <- Basis.Test[, loc1] * Basis.Test[, loc2]
}
}
temp <- exp(Mu0 + basisTest %*% Mu)
if (max(temp) > 1e+10)
temp[which(temp > 1e+10)] <- 1e+05
if (min(temp) < 1e-10)
temp[which(temp < 1e-10)] <- 1e-05
logL[i] <- mean(Target.Test * log(temp/(1 + temp)) + (1 - Target.Test) * log(1/(1 + temp)))
}
return(logL)
}
SSE1Alpha[i_s, ] <- c(-mean(logL), sd(logL)/sqrt(nFolds))
out <- c(alpha, lambda, -mean(logL), sd(logL)/sqrt(nFolds))
#names(LikelihoodStep) <- c("alpha","lambda","mean(logL)","sd(logL)")
#names(SSE1Alpha) <- c("-mean(logL)", "sd(logL)/sqrt(nFolds)")
#currentL <- -Likelihood[step, 3]
step <- step + 1
return(out)
}
#index <- which.min(SSE1Alpha[, 1])
#lambda <- Lambda[index]
#MSEeachAlpha[i_alpha, ] <- c(alpha, lambda, -SSE1Alpha[index, ])
}
cvdf <- as.data.frame(cvoutput)
colnames(cvdf) <- c("alpha", "lambda", "MSE", "StdErr")
rownames(cvdf) <- NULL
optimalindex <- which.min(cvdf$MSE)
optimalstats <- cvdf[optimalindex,]
result <- list(cvdf, optimalstats, optimalstats$alpha, optimalstats$lambda, optimalstats$MSE)
names(result) <- c("CrossValidation", "Optimal_Stats", "Alpha_Optimal", "Lambda_Optimal", "Minimum_MSE")
return(result)
}
if(prior == "gaussian"){
cat("Parallel Empirical Bayesian Elastic Net Linear Model (w/ Gaussian Prior), Epis: ", Epis, ", ", nFolds,"-fold cross-validation\n",sep="")
N <- nrow(BASIS)
K <- ncol(BASIS)
set.seed(1)
if(length(foldId)!=N){
if(N%%nFolds!=0){
foldId <- sample(c(rep(1:nFolds,floor(N/nFolds)),1:(N%%nFolds)),N)
}else{
foldId <- sample(rep(1:nFolds,floor(N/nFolds)),N)
}
}
lambda_Max <- log(1.1)
response <- Target-mean(Target)
response <- response/sqrt(sum(response*response))
for(i_b in 1:K){
basis 			= BASIS[,i_b];
basis 			= basis/sqrt(sum(basis*basis));
corBy 			= basis%*%response;
if(corBy>lambda_Max) lambda_Max = corBy;
}
if(Epis == "yes"){
for(i_b in 1:(K-1)){
for(i_bj in (i_b + 1):K){
basis 	= BASIS[,i_b]*BASIS[,i_bj];
basis 	= basis/sqrt(sum(basis*basis));
corBy 	= basis%*%(Target-mean(Target));
if(corBy>lambda_Max) lambda_Max = corBy;
}
}
}
#corBy <- foreach(i_b = 1:K, .packages = c("foreach"), .combine = "max") %dopar% {
#  basis <- BASIS[, i_b]
#  basis <- basis/sum(basis * basis)
#  corBy <- basis %*% response
#}
#if (corBy > lambda_Max){
#  lambda_Max <- corBy
#}
#if (Epis == "yes") {
#  corBy <- foreach(i_b = 1:(K - 1), .combine = "max") %dopar% {
#    foreach (i_bj = (i_b + 1):K, .combine = "max") %dopar% {
#      basis <- BASIS[, i_b] * BASIS[, i_bj]
#      basis <- basis/sqrt(sum(basis * basis))
#      corBy <- basis %*% (Target - mean(Target))
#    }
#  }
#  if(corBy > lambda_Max) {
#    lambda_Max <- corBy
#  }
#}
lambda_Max <- lambda_Max * 10;
lambda_Min <- log(0.001 * lambda_Max)
step <- (log(lambda_Max) - lambda_Min)/19
Lambda <- exp(seq(from = log(lambda_Max), to = lambda_Min, by = -step))
#lambda <- 1 #Initialize lambda
N_step <- length(Lambda)
step <- 1;
Alpha <- seq(from = 1, to = 0.05, by = -0.05)
#alpha <- 0.5 #Initialize alpha
nAlpha <- length(Alpha);
MSEcv <- mat.or.vec((N_step*nAlpha),4)
MSEeachAlpha <- mat.or.vec(nAlpha,4) # minimum MSE for each alpha
MeanSqErr <- mat.or.vec(nFolds,1)
SSE1Alpha	<- matrix(1e10,N_step,2)# temp matrix to keep MSE + std in each step
cvoutput <- foreach (i_alpha = 1:nAlpha, .combine = rbind, .packages = c("foreach")) %dopar% {
alpha <- Alpha[i_alpha]
SSE1Alpha <- matrix(1e10,N_step,2)# temp matrix to keep MSE + std in each step
cat("Testing alpha", i_alpha, "/",nAlpha,":\t\talpha: ",alpha,"\n")
foreach (i_s = 1:N_step, .combine = rbind) %dopar% {
lambda <- Lambda[i_s];
min_index <- which.min(SSE1Alpha[1:(i_s -1),1]);
previousL <- SSE1Alpha[min_index,1] + SSE1Alpha[min_index,2];
cat("\tTesting step", step, "\t\tlambda: ",lambda,"\t")
foreach (i = 1:nFolds, .combine = rbind, .packages = c("EBEN")) %dopar% {
#cat("Testing fold", j, "\n")
index <- which(foldId!=i)
Basis.Train <- BASIS[index,]
Target.Train <- Target[index]
index <- which(foldId == i)
Basis.Test <- BASIS[index,]
Target.Test <- Target[index]
SimF2fEB <- EBelasticNet.Gaussian(Basis.Train,Target.Train,lambda,alpha,Epis)
M <- length(SimF2fEB$weight)/6
Betas <- matrix(SimF2fEB$weight,nrow= M,ncol =6, byrow= FALSE)
Mu <- Betas[,3]
Mu0 <- SimF2fEB$Intercept[1]
#if(is.na(Mu0)){break}
ntest <- nrow(Basis.Test)
basisTest <- matrix(rep(0,ntest*M),ntest,M)
for(i_basis in 1:M){
loc1 <- Betas[i_basis,1]
loc2 <- Betas[i_basis,2]
if(loc1 !=0)
{
if(loc1==loc2){
basisTest[,i_basis] <- Basis.Test[,loc1]
}
else{
basisTest[,i_basis] <- Basis.Test[,loc1] * Basis.Test[,loc2]
}
}else{
basisTest <- rep(0,length(Target.Test))
}
}
#Compute Mean Square Error:
temp <- Target.Test - (Mu0 + basisTest%*%Mu)
MeanSqErr[i] <- t(temp)%*%temp
return(MeanSqErr[i])
}
SSE1Alpha[i_s,] <- c(mean(MeanSqErr),sd(MeanSqErr)/sqrt(nFolds))
cat("SSE",mean(MeanSqErr),"\n")
MSEcv[step,] <- c(alpha,lambda,mean(MeanSqErr),sd(MeanSqErr)/sqrt(nFolds))
out <- c(alpha,lambda,mean(MeanSqErr),sd(MeanSqErr)/sqrt(nFolds))
currentL <- MSEcv[step,3]
step <- step + 1;
# break out of 2nd for loop
#if((currentL - previousL)>0){break}
return(out)
}
#index <- which.min(SSE1Alpha[,1]);
#lambda <- Lambda[index];
#MSEeachAlpha[i_alpha,] <- c(alpha,lambda, SSE1Alpha[index,]);
}
cvdf <- as.data.frame(cvoutput)
colnames(cvdf) <- c("alpha", "lambda", "MSE", "StdErr")
rownames(cvdf) <- NULL
optimalindex <- which.min(cvdf$MSE)
optimalstats <- cvdf[optimalindex,]
result <- list(cvdf, optimalstats, optimalstats$alpha, optimalstats$lambda, optimalstats$MSE)
names(result) <- c("CrossValidation", "Optimal_Stats", "Alpha_Optimal", "Lambda_Optimal", "Minimum_MSE")
return(result)
}
else{
cat("Currently, only the binomial and gaussian prior distributions are allowed.")
}
}
data(BASISbinomial)
data(yBinomial)
#reduce sample size to speed up the running time
n <- 200;
k <- 300;
N <- length(yBinomial);
set.seed(1)
set  <- sample(N,n);
BASIS <- BASISbinomial[set,1:k];
y <- yBinomial[set];
nFolds <- 3
system.time(serCV <- EBelasticNet.GaussianCV(BASIS, y, nFolds = 3,Epis = "no"), gcFirst = TRUE)
system.time(parCV <- parEBEN.cv.doParallel(BASIS, y, nFolds = 3,Epis = "no", prior = "gaussian"), gcFirst = TRUE)
stopCluster(cl)
no_cores <- detectCores() - 1
cl <- makeCluster(no_cores)
clusterExport(cl, c("parEBEN.cv.doParallel"))
registerDoParallel(cl)
system.time(parCV <- parEBEN.cv.doParallel(BASIS, y, nFolds = 3,Epis = "no", prior = "gaussian"), gcFirst = TRUE)
install.packages("devtools")
library(devtools)
install_github("colbyford/parEBEN")
?EBEN
install_github("colbyford/parEBEN")
install_github("colbyford/parEBEN")
install_github("colbyford/parEBEN")
install_github("colbyford/parEBEN")
install_github("colbyford/parEBEN")
install_github("colbyford/parEBEN")
citation(package = "EBEN")
library(devtools)
install_github("colbyford/parEBEN")
library(parEBEN)
?parEBEN
?parEBEN.cv
library(doParallel)
no_cores <- detectCores() - 1
cl <- makeCluster(no_cores)
#clusterExport(cl, c("parEBEN.cv.doParallel"))
registerDoParallel(cl)
## Load in data and required EBEN and parEBEN packages
library(EBEN)
library(parEBEN)
data(BASISbinomial)
data(yBinomial)
## Create small sample matrix for testing
n <- 50
k <- 100
N <- length(yBinomial)
set.seed(1)
set <- sample(N,n)
BASIS <- BASISbinomial[set,1:k]
y <- yBinomial[set]
cv <- parEBEN.cv(BASIS, y, nFolds = 3,Epis = "no", parMethod = "doParallel", prior = "binomial")
parEBEN::parEBEN.cv()
remove.packages("parEBEN")
library(devtools)
install_github("colbyford/parEBEN")
library(parEBEN)
?parEBEN.cv
library(doParallel)
no_cores <- detectCores() - 1
cl <- makeCluster(no_cores)
#clusterExport(cl, c("parEBEN.cv.doParallel"))
registerDoParallel(cl)
## Load in data and required EBEN and parEBEN packages
library(EBEN)
library(parEBEN)
data(BASISbinomial)
data(yBinomial)
## Create small sample matrix for testing
n <- 50
k <- 100
N <- length(yBinomial)
set.seed(1)
set <- sample(N,n)
BASIS <- BASISbinomial[set,1:k]
y <- yBinomial[set]
cv <- parEBEN.cv(BASIS, y, nFolds = 3,Epis = "no", parMethod = "doParallel", prior = "binomial")
?EBEN
uninstall.packages("parEBEN")
remove.packages("parEBEN")
install_github("colbyford/parEBEN")
?parEBEN\
?parEBEN
remove.packages("parEBEN")
library(devtools)
install_github("colbyford/parEBEN")
?parEBEN
library(dplyr)
library(readr)
install.packages(readr)
install.packages("readr")
library(readr)
## Input as Table
read_csv("DimensionalityReduction_CombinedData_InnerJoin.csv")
getwd()
library(tidyr)
library(readr)
library(dplyr)
library(purrr)
library(psych)
library(MASS)
library(ggplot2)
## Input as Table
CombinedData <- read_csv("datasets/DimensionalityReduction/DimensionalityReduction_CombinedData_InnerJoin.csv") #Inner Join Data
>tidyr::separate()
?tidyr::separate
citation(package = "MASS")
library(EBEN)
BASIS
BASIS <- data(BASIS)
BASIS <- as.data.frame(data(BASIS))
View(BASIS)
data(BASIS)
View(BASIS)
install.packages("BPEC")
library(BPEC)
?BPEC
data("MacrocnemisRawSeqs")
View(MacrocnemisRawSeqs)
data("MacrocnemisCoordsLocs")
View(MacrocnemisCoordsLocs)
CoordLocs <- data("MacrocnemisCoordsLocs")
RawSeqs <- data(MacrocnemisRawSeqs)
MCMCout = BPEC.MCMC(RawSeqs,CoordsLocs,MaxMig=2,iter=50,ds=0,PostSamples=5,dims=8)
CoordsLocs <- data("MacrocnemisCoordsLocs")
MCMCout = BPEC.MCMC(RawSeqs,CoordsLocs,MaxMig=2,iter=50,ds=0,PostSamples=5,dims=8)
data(MacrocnemisRawSeqs)
data(MacrocnemisCoordsLocs)
CoordsLocs = MacrocnemisCoordsLocs
RawSeqs = MacrocnemisRawSeqs
##to use your own dataset
#RawSeqs = read.nexus.data('Haplotypes.nex')
#CoordsLocs = read.table(
# 'CoordsLocsFile.txt',header=FALSE,fill=TRUE,col.names=1:max(count.fields('CoordsLocsFile.txt'))
#  )
## to set phenotypic/environmental covariate names, use (as appropriate)
# colnames(CoordsLocs)[1:dims] = c('lat','long','cov1','cov2','cov3')
## where dims is the corresponding number of measurements available
## (2 for latitude and longitude only, add one for each additional available measurement)
#to run the MCMC sampler:
MCMCout = BPEC.MCMC(RawSeqs,CoordsLocs,MaxMig=2,iter=50,ds=0,PostSamples=5,dims=8)
BPEC.Geo = BPEC.GeoTree(MCMCout,CoordsLocs,file="GoogleEarthTree.kml")
View(BPEC.Geo)
prop.test(x = c(100,101), n = c(5000,5000))
prop.test(x = c(100,150), n = c(5000,5000))
getwd()
setwd("../Desktop/BantuMigration/Github/")
library(readxl)
ModelAcc <- read_excel("datasets/MachineLearning/MicrosoftAzureMachineLearning/RandomForest_ModelAccuracies.xlsx", sheet = 1)
prop.test(x = ModelAcc$Correct,
n = ModelAcc$Total)
